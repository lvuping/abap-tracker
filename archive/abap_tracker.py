#!/usr/bin/env python3
"""
ABAP Tracker - ê°„ì†Œí™”ëœ ë©”ì¸ ì‹¤í–‰ íŒŒì¼
"""

import os
import sys
import json
import csv
import argparse
from datetime import datetime
# from test_runner import TestRunner  # TODO: Create this module if needed
from unified_analyzer import UnifiedAnalyzer
from encoding_utils import safe_file_read


def run_test():
    """í…ŒìŠ¤íŠ¸ ëª¨ë“œ ì‹¤í–‰"""
    print("\n" + "="*80)
    print("ğŸš€ ABAP Tracker í…ŒìŠ¤íŠ¸ ì‹¤í–‰")
    print("="*80)
    
    # TODO: Implement TestRunner module
    print("âŒ TestRunner ëª¨ë“ˆì´ êµ¬í˜„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    print("ëŒ€ì‹  --batch ì˜µì…˜ì„ ì‚¬ìš©í•˜ì„¸ìš”:")
    print("  python abap_tracker.py --batch input/sy_uname_locations.csv")
    return
    
    # runner = TestRunner(verbose=False)
    # test_cases = runner.load_test_cases('input/sy_uname_locations.csv')
    # 
    # if not test_cases:
    #     print("âŒ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    #     return
    # 
    # runner.run_all_tests(test_cases)
    # 
    # # ê²°ê³¼ ì €ì¥
    # timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    # csv_file = f"output/test_results_{timestamp}.csv"
    # json_file = f"output/test_report_{timestamp}.json"
    # 
    # runner.export_csv(csv_file)
    # runner.export_json(json_file)
    # 
    # print(f"\nğŸ“Š ê²°ê³¼ íŒŒì¼:")
    # print(f"  â€¢ CSV: {csv_file}")
    # print(f"  â€¢ JSON: {json_file}")


def run_analysis(file_path, line_number, verbose=False):
    """ë‹¨ì¼ íŒŒì¼ ë¶„ì„"""
    if not os.path.exists(file_path):
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
        return None
    
    print(f"\nğŸ“ ë¶„ì„ ì¤‘: {file_path}:{line_number}")
    
    # ìë™ ì¸ì½”ë”© ê°ì§€ë¡œ íŒŒì¼ ì½ê¸°
    lines, encoding_used = safe_file_read(file_path)
    if verbose:
        print(f"  ğŸ“„ íŒŒì¼ ì¸ì½”ë”©: {encoding_used}")
    
    if line_number > len(lines):
        print(f"âŒ ë¼ì¸ {line_number}ì´ íŒŒì¼ ë²”ìœ„ë¥¼ ë²—ì–´ë‚¬ìŠµë‹ˆë‹¤.")
        return None
    
    analyzer = UnifiedAnalyzer(seed_variables=['SY-UNAME'])
    result = analyzer.analyze(lines, line_number - 1)  # 0-based index
    
    if verbose:
        print("\nğŸ“‹ ë¶„ì„ ê²°ê³¼:")
        print(f"  â€¢ ì˜¤ì—¼ëœ ë³€ìˆ˜: {len(result.get('tainted_variables', []))}")
        print(f"  â€¢ ë°ì´í„° íë¦„: {len(result.get('data_flows', []))}")
        print(f"  â€¢ DB ì‘ì—…: {len(result.get('database_sinks', []))}")
        
        if result.get('database_sinks'):
            print("\nğŸ’¾ ë°ì´í„°ë² ì´ìŠ¤ ì‘ì—…:")
            for sink in result['database_sinks'][:5]:
                fields = ', '.join(sink['fields']) if sink['fields'] else 'ALL'
                print(f"  â€¢ {sink['operation']} {sink['table']} (Line {sink['line']}, Fields: {fields})")
    
    return result


def batch_analysis(csv_file, output_format='json', verbose=False):
    """CSV íŒŒì¼ ê¸°ë°˜ ì¼ê´„ ë¶„ì„"""
    if not os.path.exists(csv_file):
        print(f"âŒ CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_file}")
        return
    
    print(f"\nğŸ“‚ ì¼ê´„ ë¶„ì„ ì‹œì‘: {csv_file}")
    
    results = []
    # CSV íŒŒì¼ ìë™ ì¸ì½”ë”© ê°ì§€
    lines, csv_encoding = safe_file_read(csv_file)
    if verbose:
        print(f"  ğŸ“„ CSV íŒŒì¼ ì¸ì½”ë”©: {csv_encoding}")
    
    import io
    csv_content = io.StringIO(''.join(lines))
    reader = csv.DictReader(csv_content)
    total = len(lines) - 1  # í—¤ë” ì œì™¸
    
    for i, row in enumerate(reader, 1):
        file_path = row.get('file_path', '').strip()
        if not file_path.endswith('.abap'):
            file_path += '.abap'
        if not file_path.startswith('input/'):
            file_path = 'input/' + file_path
        
        line_number = int(row.get('line_number', 0))
        
        if not verbose:
            print(f"  [{i}/{total}] {os.path.basename(file_path)}:{line_number}", end='\r')
        
        result = run_analysis(file_path, line_number, verbose=False)
        if result:
            results.append({
                'id': row.get('id', i),
                'file': file_path,
                'line': line_number,
                'result': result
            })
    
    print(f"\nâœ… ë¶„ì„ ì™„ë£Œ: {len(results)}ê°œ í•­ëª©")
    
    # ê²°ê³¼ ì €ì¥
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    if output_format in ['json', 'both']:
        json_file = f"output/analysis_{timestamp}.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        print(f"  â€¢ JSON ì €ì¥: {json_file}")
    
    if output_format in ['csv', 'both']:
        csv_file = f"output/analysis_{timestamp}.csv"
        with open(csv_file, 'w', newline='', encoding='utf-8') as f:
            fieldnames = ['id', 'file', 'line', 'status', 'type', 'table', 'fields']
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            
            for item in results:
                res = item['result']
                row = {
                    'id': item['id'],
                    'file': item['file'],
                    'line': item['line'],
                    'status': res.get('status', 'Unknown'),
                    'type': res.get('type', ''),
                    'table': '',
                    'fields': ''
                }
                
                if res.get('database_sinks'):
                    sink = res['database_sinks'][0]
                    row['table'] = sink['table']
                    row['fields'] = ';'.join(sink['fields'])
                
                writer.writerow(row)
        print(f"  â€¢ CSV ì €ì¥: {csv_file}")


def main():
    parser = argparse.ArgumentParser(
        description='ABAP Tracker - SY-UNAME ì¶”ì  ë° DB ì‘ì—… ë¶„ì„',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog='''
ì‚¬ìš© ì˜ˆì‹œ:
  python abap_tracker.py --test                    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
  python abap_tracker.py --file test.abap --line 10  # ë‹¨ì¼ íŒŒì¼ ë¶„ì„
  python abap_tracker.py --batch input/locations.csv # ì¼ê´„ ë¶„ì„
  python abap_tracker.py                           # ê¸°ë³¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        '''
    )
    
    parser.add_argument('--test', action='store_true', 
                       help='ì „ì²´ í…ŒìŠ¤íŠ¸ ì‹¤í–‰')
    parser.add_argument('--file', type=str,
                       help='ë¶„ì„í•  ABAP íŒŒì¼')
    parser.add_argument('--line', type=int,
                       help='SY-UNAMEì´ ìˆëŠ” ë¼ì¸ ë²ˆí˜¸')
    parser.add_argument('--batch', type=str,
                       help='ì¼ê´„ ë¶„ì„í•  CSV íŒŒì¼')
    parser.add_argument('--csv', action='store_true',
                       help='CSV í˜•ì‹ìœ¼ë¡œ ì¶œë ¥')
    parser.add_argument('--json', action='store_true',
                       help='JSON í˜•ì‹ìœ¼ë¡œ ì¶œë ¥ (ê¸°ë³¸ê°’)')
    parser.add_argument('--verbose', '-v', action='store_true',
                       help='ìƒì„¸ ì¶œë ¥')
    
    args = parser.parse_args()
    
    # ì¶œë ¥ í˜•ì‹ ê²°ì •
    output_format = 'json'  # ê¸°ë³¸ê°’
    if args.csv and args.json:
        output_format = 'both'
    elif args.csv:
        output_format = 'csv'
    
    # ì‹¤í–‰ ëª¨ë“œ ê²°ì •
    if args.test or (not args.file and not args.batch):
        # í…ŒìŠ¤íŠ¸ ëª¨ë“œ (ê¸°ë³¸)
        run_test()
    elif args.file and args.line:
        # ë‹¨ì¼ íŒŒì¼ ë¶„ì„
        result = run_analysis(args.file, args.line, args.verbose)
        if result and output_format != 'none':
            print(json.dumps(result, indent=2, ensure_ascii=False))
    elif args.batch:
        # ì¼ê´„ ë¶„ì„
        batch_analysis(args.batch, output_format, args.verbose)
    else:
        # ì¸ìê°€ ë¶€ì¡±í•œ ê²½ìš°
        if args.file and not args.line:
            print("âŒ --file ì˜µì…˜ì„ ì‚¬ìš©í•  ë•ŒëŠ” --lineë„ í•„ìš”í•©ë‹ˆë‹¤.")
        else:
            parser.print_help()


if __name__ == '__main__':
    main()